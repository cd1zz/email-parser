#!/usr/bin/env python3
import json
import logging
import sys
from pathlib import Path
sys.path.append('../../function-app')
from email_parser import create_email_parser
from urllib.parse import urlparse

# Disable logging for cleaner output
logging.disable(logging.CRITICAL)

def analyze_current_artifacts(email_file):
    """Analyze current structure to verify artifacts are correctly placed at each layer"""
    parser = create_email_parser(
        enable_url_analysis=True,
        enable_document_processing=True
    )
    
    with open(email_file, 'rb') as f:
        email_data = f.read()
    
    result = parser.parse(email_data, Path(email_file).name, verbose=False)
    
    if result.get("status") != "success":
        print(f"Error parsing {email_file}")
        return None
    
    print(f"=== CURRENT ARTIFACTS VERIFICATION: {Path(email_file).name} ===")
    
    structure = result.get("structure", {})
    email = structure.get("email", {})
    
    # Track all URLs and domains across layers
    all_urls = set()
    all_domains = set()
    
    def analyze_layer(email_obj, layer_name, level=0):
        nonlocal all_urls, all_domains
        
        print(f"\nüìß {layer_name} (Level {level}):")
        
        # Headers
        headers = email_obj.get("headers", {})
        print(f"   Headers: From={headers.get('from', 'N/A')[:50]}...")
        print(f"           Subject={headers.get('subject', 'N/A')[:50]}...")
        
        # Body artifacts
        body = email_obj.get("body", {})
        text_len = len(body.get("text", ""))
        html_len = len(body.get("html", ""))
        print(f"   Body: Text={text_len} chars, HTML={html_len} chars")
        
        # URL artifacts
        urls = email_obj.get("urls", [])
        print(f"   URLs: {len(urls)} found")
        for i, url in enumerate(urls):
            print(f"      [{i}] {url}")
            all_urls.add(url)
            try:
                domain = urlparse(url).netloc
                if domain:
                    all_domains.add(domain)
            except:
                pass
        
        # Attachment artifacts
        attachments = email_obj.get("attachments", [])
        print(f"   Attachments: {len(attachments)} found")
        for i, att in enumerate(attachments):
            print(f"      [{i}] {att.get('name', 'unnamed')} ({att.get('size', 0)} bytes)")
            print(f"          Type: {att.get('type', 'unknown')}")
            print(f"          MIME: {att.get('mime_type', 'unknown')}")
            print(f"          Contains email: {att.get('contains_email', False)}")
            
            # Check for document extracts
            doc_text = att.get('document_text')
            doc_urls = att.get('document_urls', [])
            if doc_text:
                print(f"          Document text: {len(doc_text)} chars")
            if doc_urls:
                print(f"          Document URLs: {len(doc_urls)} found")
                for doc_url in doc_urls:
                    all_urls.add(doc_url)
                    try:
                        domain = urlparse(doc_url).netloc
                        if domain:
                            all_domains.add(domain)
                    except:
                        pass
        
        # Document extracts at this layer
        doc_extracts = email_obj.get("document_extracts", [])
        if doc_extracts:
            print(f"   Document extracts: {len(doc_extracts)} found")
            for i, doc in enumerate(doc_extracts):
                print(f"      [{i}] {doc.get('source_attachment', 'unknown')} - {len(doc.get('text', ''))} chars")
                doc_urls = doc.get('urls', [])
                if doc_urls:
                    print(f"          URLs: {doc_urls}")
                    for doc_url in doc_urls:
                        all_urls.add(doc_url)
                        try:
                            domain = urlparse(doc_url).netloc
                            if domain:
                                all_domains.add(domain)
                        except:
                            pass
        
        # Process nested emails recursively
        nested_emails = email_obj.get("nested_emails", [])
        for i, nested in enumerate(nested_emails):
            analyze_layer(nested, f"NESTED-{layer_name}-{i}", level + 1)
    
    # Analyze main email
    analyze_layer(email, "MAIN-EMAIL", 0)
    
    # Show summary
    print(f"\nüåê SUMMARY:")
    print(f"   Total unique URLs: {len(all_urls)}")
    print(f"   Total unique domains: {len(all_domains)}")
    
    # Check current summary section
    current_summary = structure.get("summary", {})
    print(f"\nüìã CURRENT SUMMARY SECTION:")
    print(f"   Email chain length: {current_summary.get('email_chain_length', 0)}")
    print(f"   Domains involved: {len(current_summary.get('domains_involved', []))}")
    print(f"   Total attachments: {current_summary.get('total_attachments', 0)}")
    
    return {
        "all_urls": sorted(list(all_urls)),
        "all_domains": sorted(list(all_domains)),
        "result": result
    }

def create_proposed_structure_with_deduplication(analysis_result):
    """Create the proposed structure with proper artifact placement and deduplication"""
    
    if not analysis_result:
        return None
    
    all_urls = analysis_result["all_urls"]
    all_domains = analysis_result["all_domains"]
    result = analysis_result["result"]
    
    print(f"\n=== PROPOSED STRUCTURE WITH DEDUPLICATION ===")
    
    # Create enhanced summary with deduplication
    enhanced_summary = {
        "email_chain_length": result["structure"]["summary"]["email_chain_length"],
        "total_emails": result["structure"]["metadata"]["total_emails"],
        "total_attachments": result["structure"]["metadata"]["total_attachments"],
        "total_depth": result["structure"]["metadata"]["total_depth"],
        
        # Deduplicated URLs and domains
        "urls": {
            "total_count": len(all_urls),
            "unique_urls": all_urls,
            "shortened_urls": [{"original": url, "expanded": url} for url in all_urls if any(service in url.lower() for service in [
                "bit.ly", "t.co", "goo.gl", "ow.ly", "tinyurl.com", "is.gd", "buff.ly", 
                "rebrandly.com", "cutt.ly", "bl.ink", "snip.ly", "su.pr", "lnkd.in", 
                "fb.me", "cli.gs", "sh.st", "mcaf.ee", "yourls.org", "v.gd", "s.id", 
                "t.ly", "tiny.cc", "qlink.me", "po.st", "short.io", "shorturl.at", 
                "aka.ms", "tr.im", "bit.do", "git.io", "adf.ly", "qr.ae", "tny.im", 
                "x.co", "d.pr", "rb.gy", "vk.cc", "t1p.de", "chilp.it", "ouo.io", 
                "zi.ma", "pd.am", "hyperurl.co", "tiny.ie", "qps.ru", "l.ead.me", 
                "shorte.st"
            ])],
            "by_domain": {}
        },
        
        "domains": {
            "total_count": len(all_domains),
            "unique_domains": all_domains,
            "external_domains": [domain for domain in all_domains if not any(trusted in domain for trusted in ['microsoft.com', 'outlook.com', 'gmail.com'])],
            "by_category": {
                "email_providers": [domain for domain in all_domains if any(provider in domain for provider in ['gmail.com', 'outlook.com', 'yahoo.com'])],
                "microsoft_services": [domain for domain in all_domains if 'microsoft.com' in domain],
                "potentially_suspicious": [domain for domain in all_domains if any(suspicious in domain for suspicious in ['bit.ly', 'tinyurl', 'malicious', 'phishing'])]
            }
        },
        
        # Keep existing summary fields
        "attachment_types": result["structure"]["summary"]["attachment_types"],
        "key_subjects": result["structure"]["summary"]["key_subjects"],
        "timeline": result["structure"]["summary"]["timeline"],
        "forwarding_chain": result["structure"]["summary"]["forwarding_chain"],
        "contains_external_domains": result["structure"]["summary"]["contains_external_domains"],
        "document_summary": result["structure"]["summary"]["document_summary"]
    }
    
    # Group URLs by domain for better analysis
    for url in all_urls:
        try:
            domain = urlparse(url).netloc
            if domain:
                if domain not in enhanced_summary["urls"]["by_domain"]:
                    enhanced_summary["urls"]["by_domain"][domain] = []
                enhanced_summary["urls"]["by_domain"][domain].append(url)
        except:
            pass
    
    print("üìã ENHANCED SUMMARY STRUCTURE:")
    print(json.dumps(enhanced_summary, indent=2))
    
    return enhanced_summary

def verify_artifact_preservation():
    """Verify that all artifacts are properly preserved in the proposed structure"""
    
    print(f"\n=== ARTIFACT PRESERVATION VERIFICATION ===")
    
    print("‚úÖ PRESERVED ARTIFACTS AT EACH LAYER:")
    print("   üìß Headers: from, to, subject, date, message-id")
    print("   üìÑ Body: text, html, has_html")
    print("   üîó URLs: all URLs found in body content")
    print("   üìé Attachments: name, size, type, mime_type, contains_email")
    print("   üìÑ Document extracts: text, urls, source_attachment")
    print("   üîç Nested emails: recursive structure with full artifacts")
    
    print("\n‚úÖ STRUCTURE IMPROVEMENTS:")
    print("   üÜî Added: nested_email_id references in attachments")
    print("   üè∑Ô∏è Added: id field for each nested email")
    print("   üìç Added: source_attachment and source_attachment_index")
    print("   üî¢ Added: level field for depth tracking")
    print("   üåê Enhanced: deduplicated URLs and domains in summary")
    
    print("\n‚úÖ ELIMINATED DUPLICATION:")
    print("   ‚ùå Removed: duplicate nested_email content in attachments")
    print("   ‚úÖ Kept: single source of truth in nested_emails array")
    print("   üîó Added: references to maintain relationships")
    
    print("\n‚úÖ ENHANCED SUMMARY:")
    print("   üìä Total counts: urls, domains, emails, attachments")
    print("   üìù Deduplicated lists: unique_urls, unique_domains")
    print("   üîç Categorized: shortened_urls, external_domains")
    print("   üìã Grouped: urls_by_domain, domains_by_category")

if __name__ == "__main__":
    # Test with different email types
    test_files = [
        "../test_emails/1.msg",   # Complex email with images + nested email
        "../test_emails/5.msg",   # Complex nested with Excel attachment
    ]
    
    for test_file in test_files:
        try:
            analysis = analyze_current_artifacts(test_file)
            if analysis:
                enhanced_summary = create_proposed_structure_with_deduplication(analysis)
            print("\n" + "="*80 + "\n")
        except Exception as e:
            print(f"Error with {test_file}: {e}")
            print("\n" + "="*80 + "\n")
    
    verify_artifact_preservation()